{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53e86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.v2 as tfs\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848c8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "class DigitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e626e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss 0.47592450428142474: 100%|██████████| 938/938 [00:29<00:00, 31.86it/s]\n",
      "Epoch 2/50, Loss 0.039978800353401504:  17%|█▋        | 157/938 [00:04<00:20, 38.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m train_tqdm = tqdm(train_data, leave=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     20\u001b[39m avg_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_tqdm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:284\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:263\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Image.Image:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m         img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/PIL/Image.py:3559\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3556\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   3557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3559\u001b[39m im = \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[32m   3562\u001b[39m     checked_formats = ID.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Документы — gwyndolin/GitHub/dl/.venv/lib/python3.12/site-packages/PIL/Image.py:3546\u001b[39m, in \u001b[36mopen.<locals>._open_core\u001b[39m\u001b[34m(fp, filename, prefix, formats)\u001b[39m\n\u001b[32m   3544\u001b[39m     warning_messages.append(result)\n\u001b[32m   3545\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[32m-> \u001b[39m\u001b[32m3546\u001b[39m     \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3547\u001b[39m     im = factory(fp, filename)\n\u001b[32m   3548\u001b[39m     _decompression_bomb_check(im.size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "to_tensor = tfs.PILToTensor()\n",
    "d= ImageFolder(\"/Users/macbook/Documents/Документы — gwyndolin/GitHub/dl/MNIST/dataset/train\", transform=tfs.Compose([\n",
    "    tfs.ToTensor(), \n",
    "    tfs.Grayscale(),\n",
    "    tfs.ToDtype(torch.float32, scale=True),\n",
    "    tfs.Normalize((0.5,), (0.5,)),\n",
    "    tfs.Lambda(lambda x: x.ravel())\n",
    "]))\n",
    "train_data = data.DataLoader(d, batch_size=64, shuffle=True)\n",
    "model = DigitModel().to(device)\n",
    "model.train()\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_tqdm = tqdm(train_data, leave=True)\n",
    "    avg_loss = 0\n",
    "    for i, (x, y) in enumerate(train_tqdm):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        train_tqdm.set_description(f\"Epoch {epoch+1}/{epochs}, Loss {avg_loss/len(train_data)}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03536b",
   "metadata": {},
   "source": [
    "50 без нормализации, lr=0.001 Adam Accuracy: 0.9778\n",
    "50 с нормализацией, lr=0.001 Adam Accuracy: 0.974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d1ad435",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, \"digit_model_50_!normilize.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d1e1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "test_d= ImageFolder(\"/Users/macbook/Documents/Документы — gwyndolin/GitHub/dl/MNIST/dataset/test\", transform=tfs.Compose([\n",
    "    tfs.ToTensor(), \n",
    "    tfs.Grayscale(),\n",
    "    tfs.ToDtype(torch.float32, scale=True),\n",
    "    tfs.Normalize((0.5,), (0.5,)),\n",
    "    tfs.Lambda(lambda x: x.ravel())\n",
    "]))\n",
    "test_data = data.DataLoader(test_d, batch_size=500)\n",
    "\n",
    "Q = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(test_data):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        Q+= (y_pred == y).sum().item()\n",
    "\n",
    "Q = Q/len(test_d)\n",
    "print(f\"Accuracy: {Q}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
