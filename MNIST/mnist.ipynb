{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a53e86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir 'MNIST/runs' --port 6006\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.v2 as tfs\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848c8ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "os.system(\"rm -rf runs\")\n",
    "\n",
    "writer = SummaryWriter(\"/Users/macbook/Documents/Документы — gwyndolin/GitHub/dl/MNIST/runs\")\n",
    "\n",
    "class DigitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        #self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x)) \n",
    "        #x = self.bn2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24ae5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = tfs.Compose([\n",
    "    tfs.ToImage(),\n",
    "    tfs.Pad(4),\n",
    "    tfs.RandomCrop(28),\n",
    "    #tfs.RandomAffine(degrees=(-45, 45), translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #tfs.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    tfs.RandomAffine(degrees=(-10,10), translate=(0.05,0.05), scale=(0.95,1.05)),\n",
    "    tfs.RandomPerspective(distortion_scale=0.1, p=0.3),\n",
    "    tfs.ToDtype(torch.float32, scale=True),\n",
    "    tfs.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "val_transform = tfs.Compose([\n",
    "    tfs.ToImage(),\n",
    "    tfs.ToDtype(torch.float32, scale=True),\n",
    "    tfs.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset_full = MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=train_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "validate_dataset_full = MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=val_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_len = int(0.7 * len(train_dataset_full))\n",
    "val_len = len(train_dataset_full) - train_len\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset, _ = random_split(train_dataset_full, [train_len, val_len])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "_, validate_dataset = random_split(validate_dataset_full, [train_len, val_len])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "validate_loader = data.DataLoader(validate_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "model = DigitModel().to(device)\n",
    "model.train()\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 80\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 | Loss: 1.1551 | Acc: 0.6143: 100%|██████████| 657/657 [00:14<00:00, 45.37it/s]\n",
      "Val | Loss: 0.5719 | Acc: 0.8216: 100%|██████████| 141/141 [00:02<00:00, 68.84it/s]\n",
      "Epoch 2/80 | Loss: 0.6755 | Acc: 0.7814: 100%|██████████| 657/657 [00:11<00:00, 56.63it/s]\n",
      "Val | Loss: 0.3970 | Acc: 0.8798: 100%|██████████| 141/141 [00:02<00:00, 58.71it/s]\n",
      "Epoch 3/80 | Loss: 0.5421 | Acc: 0.8251: 100%|██████████| 657/657 [00:15<00:00, 41.32it/s]\n",
      "Val | Loss: 0.3443 | Acc: 0.8902: 100%|██████████| 141/141 [00:02<00:00, 63.99it/s]\n",
      "Epoch 4/80 | Loss: 0.4822 | Acc: 0.8459: 100%|██████████| 657/657 [00:14<00:00, 44.89it/s]\n",
      "Val | Loss: 0.3018 | Acc: 0.9043: 100%|██████████| 141/141 [00:03<00:00, 45.31it/s]\n",
      "Epoch 5/80 | Loss: 0.4360 | Acc: 0.8618: 100%|██████████| 657/657 [00:16<00:00, 38.70it/s]\n",
      "Val | Loss: 0.2661 | Acc: 0.9153: 100%|██████████| 141/141 [00:03<00:00, 46.19it/s]\n",
      "Epoch 6/80 | Loss: 0.4071 | Acc: 0.8707: 100%|██████████| 657/657 [00:15<00:00, 41.87it/s]\n",
      "Val | Loss: 0.2620 | Acc: 0.9168: 100%|██████████| 141/141 [00:02<00:00, 66.25it/s]\n",
      "Epoch 7/80 | Loss: 0.3854 | Acc: 0.8764: 100%|██████████| 657/657 [00:14<00:00, 46.82it/s]\n",
      "Val | Loss: 0.2595 | Acc: 0.9163: 100%|██████████| 141/141 [00:02<00:00, 52.22it/s]\n",
      "Epoch 8/80 | Loss: 0.3643 | Acc: 0.8829: 100%|██████████| 657/657 [00:14<00:00, 44.17it/s]\n",
      "Val | Loss: 0.2108 | Acc: 0.9362: 100%|██████████| 141/141 [00:01<00:00, 72.69it/s]\n",
      "Epoch 9/80 | Loss: 0.3458 | Acc: 0.8896: 100%|██████████| 657/657 [00:11<00:00, 58.22it/s]\n",
      "Val | Loss: 0.1944 | Acc: 0.9383: 100%|██████████| 141/141 [00:01<00:00, 71.00it/s]\n",
      "Epoch 10/80 | Loss: 0.3318 | Acc: 0.8935: 100%|██████████| 657/657 [00:10<00:00, 60.85it/s]\n",
      "Val | Loss: 0.2001 | Acc: 0.9357: 100%|██████████| 141/141 [00:01<00:00, 80.16it/s]\n",
      "Epoch 11/80 | Loss: 0.3230 | Acc: 0.8970: 100%|██████████| 657/657 [00:10<00:00, 64.18it/s]\n",
      "Val | Loss: 0.1857 | Acc: 0.9424: 100%|██████████| 141/141 [00:01<00:00, 77.85it/s]\n",
      "Epoch 12/80 | Loss: 0.3136 | Acc: 0.9007: 100%|██████████| 657/657 [00:10<00:00, 62.07it/s]\n",
      "Val | Loss: 0.1758 | Acc: 0.9452: 100%|██████████| 141/141 [00:01<00:00, 73.07it/s]\n",
      "Epoch 13/80 | Loss: 0.3098 | Acc: 0.9015: 100%|██████████| 657/657 [00:09<00:00, 65.87it/s]\n",
      "Val | Loss: 0.1641 | Acc: 0.9471: 100%|██████████| 141/141 [00:01<00:00, 79.01it/s]\n",
      "Epoch 14/80 | Loss: 0.3008 | Acc: 0.9037: 100%|██████████| 657/657 [00:10<00:00, 63.75it/s]\n",
      "Val | Loss: 0.1528 | Acc: 0.9531: 100%|██████████| 141/141 [00:01<00:00, 77.87it/s]\n",
      "Epoch 15/80 | Loss: 0.2929 | Acc: 0.9078: 100%|██████████| 657/657 [00:09<00:00, 66.85it/s]\n",
      "Val | Loss: 0.1546 | Acc: 0.9494: 100%|██████████| 141/141 [00:01<00:00, 78.21it/s]\n",
      "Epoch 16/80 | Loss: 0.2853 | Acc: 0.9085: 100%|██████████| 657/657 [00:10<00:00, 65.15it/s]\n",
      "Val | Loss: 0.1895 | Acc: 0.9369: 100%|██████████| 141/141 [00:01<00:00, 82.46it/s]\n",
      "Epoch 17/80 | Loss: 0.2811 | Acc: 0.9118: 100%|██████████| 657/657 [00:09<00:00, 65.95it/s]\n",
      "Val | Loss: 0.1552 | Acc: 0.9504: 100%|██████████| 141/141 [00:02<00:00, 56.24it/s]\n",
      "Epoch 18/80 | Loss: 0.2798 | Acc: 0.9116: 100%|██████████| 657/657 [00:12<00:00, 51.34it/s]\n",
      "Val | Loss: 0.1472 | Acc: 0.9530: 100%|██████████| 141/141 [00:01<00:00, 82.65it/s]\n",
      "Epoch 19/80 | Loss: 0.2715 | Acc: 0.9130: 100%|██████████| 657/657 [00:10<00:00, 62.05it/s]\n",
      "Val | Loss: 0.1593 | Acc: 0.9483: 100%|██████████| 141/141 [00:01<00:00, 82.10it/s]\n",
      "Epoch 20/80 | Loss: 0.2711 | Acc: 0.9142: 100%|██████████| 657/657 [00:11<00:00, 55.01it/s]\n",
      "Val | Loss: 0.1686 | Acc: 0.9490: 100%|██████████| 141/141 [00:01<00:00, 75.62it/s]\n",
      "Epoch 21/80 | Loss: 0.2645 | Acc: 0.9157: 100%|██████████| 657/657 [00:11<00:00, 59.19it/s]\n",
      "Val | Loss: 0.1550 | Acc: 0.9491: 100%|██████████| 141/141 [00:01<00:00, 77.49it/s]\n",
      "Epoch 22/80 | Loss: 0.2606 | Acc: 0.9167: 100%|██████████| 657/657 [00:10<00:00, 62.31it/s]\n",
      "Val | Loss: 0.1411 | Acc: 0.9534: 100%|██████████| 141/141 [00:02<00:00, 68.69it/s]\n",
      "Epoch 23/80 | Loss: 0.2627 | Acc: 0.9179: 100%|██████████| 657/657 [00:12<00:00, 51.95it/s]\n",
      "Val | Loss: 0.1641 | Acc: 0.9473: 100%|██████████| 141/141 [00:01<00:00, 79.93it/s]\n",
      "Epoch 24/80 | Loss: 0.2531 | Acc: 0.9202: 100%|██████████| 657/657 [00:11<00:00, 59.02it/s]\n",
      "Val | Loss: 0.1480 | Acc: 0.9523: 100%|██████████| 141/141 [00:02<00:00, 70.32it/s]\n",
      "Epoch 25/80 | Loss: 0.2533 | Acc: 0.9199: 100%|██████████| 657/657 [00:11<00:00, 57.66it/s]\n",
      "Val | Loss: 0.1570 | Acc: 0.9513: 100%|██████████| 141/141 [00:01<00:00, 72.32it/s]\n",
      "Epoch 26/80 | Loss: 0.2490 | Acc: 0.9222: 100%|██████████| 657/657 [00:11<00:00, 58.17it/s]\n",
      "Val | Loss: 0.1500 | Acc: 0.9494: 100%|██████████| 141/141 [00:01<00:00, 81.31it/s]\n",
      "Epoch 27/80 | Loss: 0.2454 | Acc: 0.9217: 100%|██████████| 657/657 [00:12<00:00, 53.63it/s]\n",
      "Val | Loss: 0.1370 | Acc: 0.9564: 100%|██████████| 141/141 [00:02<00:00, 66.61it/s]\n",
      "Epoch 28/80 | Loss: 0.2426 | Acc: 0.9219: 100%|██████████| 657/657 [00:11<00:00, 58.87it/s]\n",
      "Val | Loss: 0.1332 | Acc: 0.9568: 100%|██████████| 141/141 [00:01<00:00, 78.79it/s]\n",
      "Epoch 29/80 | Loss: 0.2437 | Acc: 0.9218: 100%|██████████| 657/657 [00:10<00:00, 62.51it/s] \n",
      "Val | Loss: 0.1229 | Acc: 0.9610: 100%|██████████| 141/141 [00:01<00:00, 81.48it/s]\n",
      "Epoch 30/80 | Loss: 0.2388 | Acc: 0.9235: 100%|██████████| 657/657 [00:11<00:00, 58.01it/s]\n",
      "Val | Loss: 0.1264 | Acc: 0.9582: 100%|██████████| 141/141 [00:02<00:00, 65.91it/s]\n",
      "Epoch 31/80 | Loss: 0.2365 | Acc: 0.9260: 100%|██████████| 657/657 [00:17<00:00, 38.01it/s]\n",
      "Val | Loss: 0.1304 | Acc: 0.9579: 100%|██████████| 141/141 [00:02<00:00, 49.49it/s]\n",
      "Epoch 32/80 | Loss: 0.2400 | Acc: 0.9237: 100%|██████████| 657/657 [00:15<00:00, 41.72it/s]\n",
      "Val | Loss: 0.1391 | Acc: 0.9566: 100%|██████████| 141/141 [00:02<00:00, 64.25it/s]\n",
      "Epoch 33/80 | Loss: 0.2352 | Acc: 0.9245: 100%|██████████| 657/657 [00:14<00:00, 46.04it/s]\n",
      "Val | Loss: 0.1375 | Acc: 0.9548: 100%|██████████| 141/141 [00:02<00:00, 60.98it/s]\n",
      "Epoch 34/80 | Loss: 0.2271 | Acc: 0.9282: 100%|██████████| 657/657 [00:13<00:00, 47.01it/s]\n",
      "Val | Loss: 0.1284 | Acc: 0.9604: 100%|██████████| 141/141 [00:01<00:00, 79.32it/s]\n",
      "Epoch 35/80 | Loss: 0.2273 | Acc: 0.9277: 100%|██████████| 657/657 [00:11<00:00, 59.64it/s]\n",
      "Val | Loss: 0.1326 | Acc: 0.9584: 100%|██████████| 141/141 [00:02<00:00, 67.82it/s]\n",
      "Epoch 36/80 | Loss: 0.2231 | Acc: 0.9286: 100%|██████████| 657/657 [00:11<00:00, 57.31it/s]\n",
      "Val | Loss: 0.1163 | Acc: 0.9619: 100%|██████████| 141/141 [00:01<00:00, 81.64it/s]\n",
      "Epoch 37/80 | Loss: 0.2302 | Acc: 0.9279: 100%|██████████| 657/657 [00:11<00:00, 57.28it/s]\n",
      "Val | Loss: 0.1163 | Acc: 0.9630: 100%|██████████| 141/141 [00:01<00:00, 80.67it/s]\n",
      "Epoch 38/80 | Loss: 0.2234 | Acc: 0.9293: 100%|██████████| 657/657 [00:10<00:00, 60.59it/s]\n",
      "Val | Loss: 0.1342 | Acc: 0.9566: 100%|██████████| 141/141 [00:01<00:00, 81.77it/s]\n",
      "Epoch 39/80 | Loss: 0.2242 | Acc: 0.9292: 100%|██████████| 657/657 [00:11<00:00, 57.48it/s]\n",
      "Val | Loss: 0.1252 | Acc: 0.9599: 100%|██████████| 141/141 [00:01<00:00, 82.22it/s]\n",
      "Epoch 40/80 | Loss: 0.2241 | Acc: 0.9295: 100%|██████████| 657/657 [00:10<00:00, 61.98it/s]\n",
      "Val | Loss: 0.1125 | Acc: 0.9641: 100%|██████████| 141/141 [00:01<00:00, 76.04it/s]\n",
      "Epoch 41/80 | Loss: 0.2185 | Acc: 0.9292: 100%|██████████| 657/657 [00:10<00:00, 63.23it/s]\n",
      "Val | Loss: 0.1173 | Acc: 0.9627: 100%|██████████| 141/141 [00:01<00:00, 77.04it/s]\n",
      "Epoch 42/80 | Loss: 0.2210 | Acc: 0.9300: 100%|██████████| 657/657 [00:11<00:00, 57.71it/s]\n",
      "Val | Loss: 0.1138 | Acc: 0.9635: 100%|██████████| 141/141 [00:02<00:00, 65.01it/s]\n",
      "Epoch 43/80 | Loss: 0.2185 | Acc: 0.9298: 100%|██████████| 657/657 [00:12<00:00, 51.43it/s]\n",
      "Val | Loss: 0.1233 | Acc: 0.9602: 100%|██████████| 141/141 [00:01<00:00, 80.36it/s]\n",
      "Epoch 44/80 | Loss: 0.2161 | Acc: 0.9308: 100%|██████████| 657/657 [00:11<00:00, 56.14it/s]\n",
      "Val | Loss: 0.1190 | Acc: 0.9624: 100%|██████████| 141/141 [00:01<00:00, 79.08it/s]\n",
      "Epoch 45/80 | Loss: 0.2121 | Acc: 0.9311: 100%|██████████| 657/657 [00:11<00:00, 57.88it/s]\n",
      "Val | Loss: 0.1119 | Acc: 0.9653: 100%|██████████| 141/141 [00:01<00:00, 76.64it/s]\n",
      "Epoch 46/80 | Loss: 0.2138 | Acc: 0.9328: 100%|██████████| 657/657 [00:11<00:00, 55.92it/s]\n",
      "Val | Loss: 0.1227 | Acc: 0.9614: 100%|██████████| 141/141 [00:01<00:00, 81.06it/s]\n",
      "Epoch 47/80 | Loss: 0.2149 | Acc: 0.9318: 100%|██████████| 657/657 [00:12<00:00, 53.42it/s]\n",
      "Val | Loss: 0.1212 | Acc: 0.9621: 100%|██████████| 141/141 [00:01<00:00, 82.50it/s]\n",
      "Epoch 48/80 | Loss: 0.2120 | Acc: 0.9325: 100%|██████████| 657/657 [00:10<00:00, 60.92it/s]\n",
      "Val | Loss: 0.1189 | Acc: 0.9632: 100%|██████████| 141/141 [00:01<00:00, 76.80it/s]\n",
      "Epoch 49/80 | Loss: 0.2100 | Acc: 0.9330: 100%|██████████| 657/657 [00:10<00:00, 60.71it/s]\n",
      "Val | Loss: 0.1325 | Acc: 0.9581: 100%|██████████| 141/141 [00:01<00:00, 80.77it/s]\n",
      "Epoch 50/80 | Loss: 0.2062 | Acc: 0.9343: 100%|██████████| 657/657 [00:10<00:00, 61.20it/s]\n",
      "Val | Loss: 0.1107 | Acc: 0.9657: 100%|██████████| 141/141 [00:01<00:00, 79.19it/s]\n",
      "Epoch 51/80 | Loss: 0.2050 | Acc: 0.9336: 100%|██████████| 657/657 [00:10<00:00, 61.26it/s]\n",
      "Val | Loss: 0.1174 | Acc: 0.9646: 100%|██████████| 141/141 [00:01<00:00, 78.45it/s]\n",
      "Epoch 52/80 | Loss: 0.2070 | Acc: 0.9349: 100%|██████████| 657/657 [00:11<00:00, 56.58it/s]\n",
      "Val | Loss: 0.1311 | Acc: 0.9581: 100%|██████████| 141/141 [00:02<00:00, 69.46it/s]\n",
      "Epoch 53/80 | Loss: 0.2068 | Acc: 0.9341: 100%|██████████| 657/657 [00:11<00:00, 59.03it/s]\n",
      "Val | Loss: 0.1028 | Acc: 0.9681: 100%|██████████| 141/141 [00:01<00:00, 82.71it/s]\n",
      "Epoch 54/80 | Loss: 0.2082 | Acc: 0.9331: 100%|██████████| 657/657 [00:10<00:00, 60.30it/s]\n",
      "Val | Loss: 0.1131 | Acc: 0.9631: 100%|██████████| 141/141 [00:01<00:00, 82.87it/s]\n",
      "Epoch 55/80 | Loss: 0.2033 | Acc: 0.9357: 100%|██████████| 657/657 [00:10<00:00, 63.77it/s]\n",
      "Val | Loss: 0.1062 | Acc: 0.9658: 100%|██████████| 141/141 [00:01<00:00, 78.64it/s]\n",
      "Epoch 56/80 | Loss: 0.2088 | Acc: 0.9344: 100%|██████████| 657/657 [00:10<00:00, 60.22it/s]\n",
      "Val | Loss: 0.1022 | Acc: 0.9684: 100%|██████████| 141/141 [00:01<00:00, 77.58it/s]\n",
      "Epoch 57/80 | Loss: 0.1979 | Acc: 0.9371: 100%|██████████| 657/657 [00:11<00:00, 59.25it/s]\n",
      "Val | Loss: 0.1091 | Acc: 0.9655: 100%|██████████| 141/141 [00:02<00:00, 65.53it/s]\n",
      "Epoch 58/80 | Loss: 0.2032 | Acc: 0.9349: 100%|██████████| 657/657 [00:10<00:00, 61.46it/s]\n",
      "Val | Loss: 0.1179 | Acc: 0.9624: 100%|██████████| 141/141 [00:01<00:00, 82.00it/s]\n",
      "Epoch 59/80 | Loss: 0.2051 | Acc: 0.9354: 100%|██████████| 657/657 [00:12<00:00, 51.50it/s]\n",
      "Val | Loss: 0.1296 | Acc: 0.9598: 100%|██████████| 141/141 [00:02<00:00, 66.07it/s]\n",
      "Epoch 60/80 | Loss: 0.1945 | Acc: 0.9381: 100%|██████████| 657/657 [00:13<00:00, 48.78it/s]\n",
      "Val | Loss: 0.1017 | Acc: 0.9677: 100%|██████████| 141/141 [00:01<00:00, 72.53it/s]\n",
      "Epoch 61/80 | Loss: 0.1980 | Acc: 0.9374: 100%|██████████| 657/657 [00:12<00:00, 53.54it/s]\n",
      "Val | Loss: 0.1372 | Acc: 0.9547: 100%|██████████| 141/141 [00:01<00:00, 79.20it/s]\n",
      "Epoch 62/80 | Loss: 0.2007 | Acc: 0.9356: 100%|██████████| 657/657 [00:12<00:00, 54.41it/s]\n",
      "Val | Loss: 0.1102 | Acc: 0.9652: 100%|██████████| 141/141 [00:01<00:00, 81.26it/s]\n",
      "Epoch 63/80 | Loss: 0.1989 | Acc: 0.9365: 100%|██████████| 657/657 [00:16<00:00, 39.47it/s]\n",
      "Val | Loss: 0.0969 | Acc: 0.9698: 100%|██████████| 141/141 [00:02<00:00, 47.25it/s]\n",
      "Epoch 64/80 | Loss: 0.2013 | Acc: 0.9363: 100%|██████████| 657/657 [00:14<00:00, 44.49it/s]\n",
      "Val | Loss: 0.1131 | Acc: 0.9635: 100%|██████████| 141/141 [00:02<00:00, 70.30it/s]\n",
      "Epoch 65/80 | Loss: 0.2004 | Acc: 0.9377: 100%|██████████| 657/657 [00:14<00:00, 46.71it/s]\n",
      "Val | Loss: 0.1058 | Acc: 0.9662: 100%|██████████| 141/141 [00:02<00:00, 65.60it/s]\n",
      "Epoch 66/80 | Loss: 0.1976 | Acc: 0.9368: 100%|██████████| 657/657 [00:16<00:00, 39.30it/s]\n",
      "Val | Loss: 0.1079 | Acc: 0.9658: 100%|██████████| 141/141 [00:01<00:00, 76.78it/s]\n",
      "Epoch 67/80 | Loss: 0.1968 | Acc: 0.9395: 100%|██████████| 657/657 [00:12<00:00, 53.91it/s]\n",
      "Val | Loss: 0.1273 | Acc: 0.9611: 100%|██████████| 141/141 [00:01<00:00, 78.10it/s]\n",
      "Epoch 68/80 | Loss: 0.1949 | Acc: 0.9395: 100%|██████████| 657/657 [00:12<00:00, 54.29it/s]\n",
      "Val | Loss: 0.1140 | Acc: 0.9627: 100%|██████████| 141/141 [00:01<00:00, 71.76it/s]\n",
      "Epoch 69/80 | Loss: 0.1973 | Acc: 0.9373: 100%|██████████| 657/657 [00:12<00:00, 53.28it/s]\n",
      "Val | Loss: 0.1084 | Acc: 0.9663: 100%|██████████| 141/141 [00:03<00:00, 38.31it/s]\n",
      "Epoch 70/80 | Loss: 0.1976 | Acc: 0.9381: 100%|██████████| 657/657 [00:15<00:00, 41.94it/s]\n",
      "Val | Loss: 0.0951 | Acc: 0.9709: 100%|██████████| 141/141 [00:02<00:00, 68.61it/s]\n",
      "Epoch 71/80 | Loss: 0.1935 | Acc: 0.9390: 100%|██████████| 657/657 [00:13<00:00, 47.03it/s]\n",
      "Val | Loss: 0.1003 | Acc: 0.9701: 100%|██████████| 141/141 [00:01<00:00, 81.42it/s]\n",
      "Epoch 72/80 | Loss: 0.1989 | Acc: 0.9368: 100%|██████████| 657/657 [00:11<00:00, 55.80it/s]\n",
      "Val | Loss: 0.1099 | Acc: 0.9649: 100%|██████████| 141/141 [00:02<00:00, 58.43it/s]\n",
      "Epoch 73/80 | Loss: 0.1962 | Acc: 0.9387: 100%|██████████| 657/657 [00:13<00:00, 48.01it/s]\n",
      "Val | Loss: 0.0994 | Acc: 0.9684: 100%|██████████| 141/141 [00:01<00:00, 74.79it/s]\n",
      "Epoch 74/80 | Loss: 0.1936 | Acc: 0.9385: 100%|██████████| 657/657 [00:11<00:00, 57.66it/s]\n",
      "Val | Loss: 0.1152 | Acc: 0.9628: 100%|██████████| 141/141 [00:01<00:00, 80.65it/s]\n",
      "Epoch 75/80 | Loss: 0.1905 | Acc: 0.9389: 100%|██████████| 657/657 [00:09<00:00, 66.41it/s]\n",
      "Val | Loss: 0.1041 | Acc: 0.9688: 100%|██████████| 141/141 [00:01<00:00, 81.65it/s]\n",
      "Epoch 76/80 | Loss: 0.1923 | Acc: 0.9385: 100%|██████████| 657/657 [00:09<00:00, 68.10it/s]\n",
      "Val | Loss: 0.1243 | Acc: 0.9610: 100%|██████████| 141/141 [00:01<00:00, 82.39it/s]\n",
      "Epoch 77/80 | Loss: 0.1860 | Acc: 0.9417: 100%|██████████| 657/657 [00:09<00:00, 67.41it/s]\n",
      "Val | Loss: 0.1130 | Acc: 0.9633: 100%|██████████| 141/141 [00:01<00:00, 82.81it/s]\n",
      "Epoch 78/80 | Loss: 0.1926 | Acc: 0.9394: 100%|██████████| 657/657 [00:09<00:00, 70.52it/s]\n",
      "Val | Loss: 0.1090 | Acc: 0.9663: 100%|██████████| 141/141 [00:01<00:00, 82.06it/s]\n",
      "Epoch 79/80 | Loss: 0.1945 | Acc: 0.9383: 100%|██████████| 657/657 [00:09<00:00, 70.88it/s]\n",
      "Val | Loss: 0.1046 | Acc: 0.9676: 100%|██████████| 141/141 [00:01<00:00, 81.68it/s]\n",
      "Epoch 80/80 | Loss: 0.1917 | Acc: 0.9399: 100%|██████████| 657/657 [00:09<00:00, 71.41it/s]\n",
      "Val | Loss: 0.1016 | Acc: 0.9676: 100%|██████████| 141/141 [00:01<00:00, 82.80it/s]\n"
     ]
    }
   ],
   "source": [
    "os.system(\"rm -rf MNIST/checkpoints\")\n",
    "os.makedirs(\"MNIST/checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---------- TRAIN ----------\n",
    "    model.train()\n",
    "    train_tqdm = tqdm(train_loader, leave=True)\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(train_tqdm):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # статистика\n",
    "        train_loss += loss.item()\n",
    "        preds = y_pred.argmax(dim=1)\n",
    "        train_correct += (preds == y).sum().item()\n",
    "        train_total += len(y)\n",
    "\n",
    "        avg_train_loss = train_loss / (i + 1)\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        train_tqdm.set_description(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        validate_tqdm = tqdm(validate_loader, leave=True)\n",
    "        for i, (x, y) in enumerate(validate_tqdm):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = y_pred.argmax(dim=1)\n",
    "            val_correct += (preds == y).sum().item()\n",
    "            val_total += len(y)\n",
    "\n",
    "            avg_val_loss = val_loss / (i + 1)\n",
    "            batch_acc = val_correct / val_total\n",
    "            validate_tqdm.set_description(f\"Val | Loss: {avg_val_loss:.4f} | Acc: {batch_acc:.4f}\")\n",
    "\n",
    "    # ---------- Metrics ----------\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(validate_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # ---------- TensorBoard ----------\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    # ---------- Checkpoints ----------\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"MNIST/checkpoints/mnist_epoch_{epoch}.pt\")\n",
    "\n",
    "    #print(f\"\\n✅ Epoch {epoch+1}/{epochs} | \"\n",
    "    #      f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "    #      f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c790f1d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwriter\u001b[49m.close()\n",
      "\u001b[31mNameError\u001b[39m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03536b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ad435",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, \"MNIST/finish/mnist.tar\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d1e1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "# --- Трансформации для теста ---\n",
    "test_transform = tfs.Compose([\n",
    "    tfs.ToImage(),\n",
    "    tfs.ToDtype(torch.float32, scale=True),\n",
    "    tfs.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# --- MNIST test (автоматическая загрузка) ---\n",
    "test_d = MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=test_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# --- DataLoader ---\n",
    "test_data = data.DataLoader(test_d, batch_size=500, shuffle=False)\n",
    "\n",
    "# --- Оценка точности ---\n",
    "model.eval()\n",
    "Q = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_data:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        Q += (y_pred == y).sum().item()\n",
    "\n",
    "Q /= len(test_d)\n",
    "print(f\"Accuracy: {Q:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
